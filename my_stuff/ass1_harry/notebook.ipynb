{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Guidelines\n",
    "\n",
    "> Remember that this is a code notebook - add an explanation of what you do using text boxes and markdown, and comment your code. Answers without explanations may get less points.\n",
    ">\n",
    "> If you re-use a substantial portion of code you find online, e.g on Stackoverflow, you need to add a link to it and make the borrowing explicit. The same applies of you take it and modify it, even substantially. There is nothing bad in doing that, providing you are acknowledging it and make it clear you know what you're doing.\n",
    ">\n",
    "> The **Generative AI policy** from the syllabus for the programming assignments applies. Generative AI can be used as a source of information in these assignments if properly referenced. You can use generative AI assistance for writing code, but you must reference the chat used as a source, just as if you would take from StackOverflow. In ChatGPT, you can make an URL to the information you obtained by clicking the \"Share link to Chat\" button and then \"Copy Link\". This allows you to cite the source of the information you use in your answer or code solution. Of course, as you know, GenAI tools are not always a reliable source and its answers are intransparantly drawn from other sources - it is recommended to cross-check its output with other sources or your own understanding of the topic.\n",
    "> \n",
    "> For the explanations of what you do that you provide with each question, as well as for (sub)questions that ask about things like motivation of choices or your opinion, the answer to this must be conceptualized and written by yourself and not copied from a generative AI source.\n",
    ">\n",
    "> Make sure your notebooks have been run when you submit, as I won't run them myself. Submit both the `.ipynb` file along with an `.html` export of the same. Submit all necessary auxilliary files as well. Please compress your submission into a `.zip` archive. Only `.zip` files can be submitted.\n",
    "> If you are using Google Colab, here is a tutorial for obtaining an HTML export: https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab .\n",
    ">\n",
    "> With Jupyter, you can simply export it as HTML through the File menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grading policy\n",
    "> As follows:\n",
    ">\n",
    "> * 70 points for correctly completing the assignment.\n",
    ">\n",
    "> * 20 points for appropriately writing and organizing your code in terms of structure, readibility (also by humans), comments and minimal documentation. It is important to be concise but also to explain what you did and why, when not obvious. Feel free to re-use functions and variables from previous questions if that helps for structure and readability - you do not need to repeat previous steps for each question.\n",
    "> \n",
    "> * 10 points for doing something extra, e.g., if you go beyond expectations (overall or on something specific). Some ideas for extras might be mentioned in the exercises, or you can come up with your own. You don't need to do them all to get the bonus. The sum of points is 90, doing (some of) the extras can bring you to 100, so the extras are not necessary to get an A.\n",
    "> \n",
    "\n",
    "**The AUC code of conduct applies to this assignment: please only submit your own work and follow the instructions on referencing external sources above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm up (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2 points)\n",
    "\n",
    "Explain why `list1` and `list2` behave differently when they are passed to the `append_to_nested_list()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Python'], [], []]\n",
      "[['Python'], ['Python'], ['Python']]\n"
     ]
    }
   ],
   "source": [
    "def append_to_nested_list(a_list):\n",
    "    a_list[0].append(\"Python\")\n",
    "    return a_list\n",
    "    \n",
    "list1 = [[], [], []]\n",
    "list2 = [[]] * 3\n",
    "\n",
    "print(append_to_nested_list(list1))\n",
    "print(append_to_nested_list(list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> EXPLANATION </b>\n",
    "\n",
    "In `list1`, three distinct nested lists are created, so appending to one does not affect the others.\n",
    "In `list2`, however, only one nested list is created in memory, and is pointed to three times. Basically, `list2[0]` is the same object as `list2[1]` and `list2[2]`, so appending to `list2[0]` also appends to the other nested lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (2 points)\n",
    "\n",
    "Write a function that counts the **total** frequency of words that start and end with the same character (e.g. comic) in a text file and test it on `data/melville-md.txt`. Total frequency means that you end with one number, although you are encouraged to show intermediate steps.\n",
    "\n",
    "Ensure that the words are treated case-insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12688\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def freq_start_end_same(file_name: str) -> int:\n",
    "    '''\n",
    "    DOCSTRING HERE\n",
    "    '''\n",
    "    count = 0\n",
    "    with open(file_name) as file:\n",
    "        for line in file.readlines():\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word.lower()[0] == word.lower()[-1] and word.isalpha():\n",
    "                    # print(word)\n",
    "                    count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(freq_start_end_same(\"data/melville-md.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> EXPLANATION </b>\n",
    "\n",
    "A counter is initialised to 0. We open the file, and for every line, we split it into a list of individual words. Then, for every word, if it only contains alphabetic letters and its first and last characters are the same, we increment the counter by 1. Finally, we return the counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (2 points)\n",
    "\n",
    "Rewrite the following code such that:\n",
    "\n",
    "- the sequence of fruit names are randomly presented (without replacement). Use a function in the [random](https://docs.python.org/3.7/library/random.html) module for this.\n",
    "\n",
    "\n",
    "- the article \"an\" is used when a fruit name begins with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a apple\n",
      "We have a apricot\n",
      "We have a avocado\n",
      "We have a banana\n",
      "We have a bilberry\n",
      "We have a blackberry\n",
      "We have a blackcurrant\n",
      "We have a blueberry\n",
      "We have a boysenberry\n",
      "We have a currant\n",
      "We have a cherry\n",
      "We have a cherimoya\n",
      "We have a cloudberry\n",
      "We have a coconut\n",
      "We have a cranberry\n",
      "We have a cucumber\n",
      "We have a damson\n",
      "We have a date\n",
      "We have a dragonfruit\n",
      "We have a durian\n",
      "We have a elderberry\n",
      "We have a feijoa\n",
      "We have a fig\n",
      "We have a gooseberry\n",
      "We have a grape\n",
      "We have a raisin\n",
      "We have a grapefruit\n",
      "We have a guava\n",
      "We have a honeyberry\n",
      "We have a huckleberry\n",
      "We have a jabuticaba\n",
      "We have a jackfruit\n",
      "We have a jambul\n",
      "We have a jujube\n",
      "We have a kiwano\n",
      "We have a kiwifruit\n",
      "We have a kumquat\n",
      "We have a lemon\n",
      "We have a lime\n",
      "We have a loquat\n",
      "We have a longan\n",
      "We have a lychee\n",
      "We have a mango\n",
      "We have a mangosteen\n",
      "We have a marionberry\n",
      "We have a melon\n",
      "We have a cantaloupe\n",
      "We have a honeydew\n",
      "We have a watermelon\n",
      "We have a mulberry\n",
      "We have a nectarine\n",
      "We have a nance\n",
      "We have a orange\n",
      "We have a clementine\n",
      "We have a mandarine\n",
      "We have a tangerine\n",
      "We have a papaya\n",
      "We have a passionfruit\n",
      "We have a peach\n",
      "We have a pear\n",
      "We have a persimmon\n",
      "We have a physalis\n",
      "We have a plantain\n",
      "We have a plum\n",
      "We have a prune\n",
      "We have a pineapple\n",
      "We have a plumcot\n",
      "We have a pomegranate\n",
      "We have a pomelo\n",
      "We have a quince\n",
      "We have a raspberry\n",
      "We have a salmonberry\n",
      "We have a rambutan\n",
      "We have a redcurrant\n",
      "We have a salak\n",
      "We have a satsuma\n",
      "We have a soursop\n",
      "We have a strawberry\n",
      "We have a tamarillo\n",
      "We have a tamarind\n",
      "We have a yuzu\n"
     ]
    }
   ],
   "source": [
    "available_fruit = ['apple', 'apricot', 'avocado', 'banana', 'bilberry', 'blackberry', 'blackcurrant', 'blueberry', 'boysenberry', 'currant', 'cherry', 'cherimoya', 'cloudberry', 'coconut', 'cranberry', 'cucumber', 'damson', 'date', 'dragonfruit', 'durian', 'elderberry', 'feijoa', 'fig', 'gooseberry', 'grape', 'raisin', 'grapefruit', 'guava', 'honeyberry', 'huckleberry', 'jabuticaba', 'jackfruit', 'jambul', 'jujube', 'kiwano', 'kiwifruit', 'kumquat', 'lemon', 'lime', 'loquat', 'longan', 'lychee', 'mango', 'mangosteen', 'marionberry', 'melon', 'cantaloupe', 'honeydew', 'watermelon', 'mulberry', 'nectarine', 'nance', 'orange', 'clementine', 'mandarine', 'tangerine', 'papaya', 'passionfruit', 'peach', 'pear', 'persimmon', 'physalis', 'plantain', 'plum', 'prune', 'pineapple', 'plumcot', 'pomegranate', 'pomelo', 'quince', 'raspberry', 'salmonberry', 'rambutan', 'redcurrant', 'salak', 'satsuma', 'soursop', 'strawberry', 'tamarillo', 'tamarind', 'yuzu']\n",
    "\n",
    "for fruit in available_fruit:\n",
    "    print(\"We have a \" + fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a melancia\n",
      "We have a morango\n",
      "We have an abacate\n",
      "We have a mirtilo\n",
      "We have a cereja\n",
      "We have an ananás\n",
      "We have a framboesa\n",
      "We have a maçã\n",
      "We have an uva\n",
      "We have a lima\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from random import randrange\n",
    "\n",
    "def rand_fruit(fruits: list) -> None:\n",
    "    '''\n",
    "    DOCSTRING\n",
    "    '''\n",
    "    while fruits:\n",
    "        fruit = fruits.pop(randrange(len(fruits)))\n",
    "        article = \"an\" if fruit[0] in \"aeioui\" else \"a\"\n",
    "        print(f\"We have {article} {fruit}\")\n",
    "\n",
    "fruits_pt = [\n",
    "    \"abacate\", \"ananás\", \"cereja\", \"framboesa\", \"lima\", \"maçã\", \"melancia\", \"mirtilo\", \"morango\", \"uva\"\n",
    "]\n",
    "rand_fruit(fruits_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> EXPLANATION </b>\n",
    "\n",
    "I chose to use the `randrange()` function from the `random` library to get a random index from 0 to `len(fruits)` and pop the element at said index. An if condition calculates if the article should be \"a\" or \"an\" based on the first letter of the element, which is then formatted into the desired output. This repeats until all elements have been popped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5 points)\n",
    "\n",
    "The following code has been written to extract all word-initial consonant clusters in a text (e.g. \"br\" in \"bread). Each sequence is obtained by matching any sequence of letters that does not include 'aeiou' and that occurs after a whitespace or the start of the line and that consists of 2 or more such characters.\n",
    "\n",
    "It works by reading an input file line by line, and finding all matches of a regular expression in this line (case insensitive).\n",
    "\n",
    "Unfortunately, the method only counts, and we do not find out which word-initial consonants are present in the text. Can you find a way to save all matching consonant clusters to the dictionary named \"consonantclusters\" with their frequency as the value, and then print this dictionary? Note that there can be multiple results per line. Try to avoid capturing the space(s) before the consonant cluster also. As for every question, explain what you did and how your solution works.\n",
    "\n",
    "Solutions where you adapt the provided regular expression will get more points than non-regex solutions, but you can try a non-regex solution if you are stuck.\n",
    "\n",
    "**Possible extra:** Print the consonant clusters sorted by frequency and in a nice looking way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51523\n"
     ]
    }
   ],
   "source": [
    "consonantclusters = {}\n",
    "consonantclustercount = 0\n",
    "with codecs.open(\"data/melville-md.txt\", \"r\", encoding=\"utf8\") as infile:\n",
    "    consonantclusterregex = re.compile(r'(^|\\s)(?:(?![aeiouy])[a-z]){2,}')\n",
    "    for line in infile:\n",
    "        result = consonantclusterregex.findall(line.lower())\n",
    "        if result:\n",
    "            consonantclustercount += len(result)\n",
    "print(consonantclustercount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (9 points)\n",
    "\n",
    "Please use the frequencies in `late_arrival_causes` to create a duplicate of the plot below, as close as possible. This is called a Pareto chart.\n",
    "\n",
    "Note: the line plot above the bars shows the cumulative frequency.\n",
    "\n",
    "**Possible extra:** suggest, motivate and implement an alternative visualization for the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pareto chart](images/pareto-chart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_arrival_causes = {\"Child Care\" : 44, \"Emergency\" : 7, \"Overslept\" : 11, \"Traffic\" : 56, \"Transp.\" : 27, \"Weather\" : 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipelines (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (20 points)\n",
    "\n",
    "- Download a 19th-century edition (or earlier, but not later!) of a book you like from the [Internet Archive](https://archive.org) in `.txt` format. For example, [Frankenstein](https://archive.org/details/ghostseer01schiuoft/page/n6). Add the link to the edition you used to your answer, as well as the `.txt` file to your submission.\n",
    "\n",
    "- Write code that:\n",
    "\n",
    "    1. Reads the text in memory.\n",
    "    \n",
    "    1. Pre-processes the text in a way that suits this type of data. One step is typically tokenization, for which you can use a tokenizer from [NLTK](https://www.nltk.org/api/nltk.tokenize.html) and optionally other preprocessing steps if you feel this helps. To get full points, you should motivate your choice of tokenizer and choice of other preprocessing steps (or lack thereof).\n",
    "    \n",
    "    1. Filter out words that consist of strictly less than 4 alphabetic characters.\n",
    "\n",
    "    1. Counts the frequencies of all the words in the corpus (words should be counted case-insensitive).\n",
    "\n",
    "    1. Writes each word-frequency pair to a csv file (from most frequent to rarest).\n",
    "\n",
    "Comment on your results, especially looking at very frequent and very infrequent words. What is problematic about processing these old editions? Can you find some limitations of the tokenizer in use, and think about how you would improve on it? Naturally, this part is required for full points.\n",
    "\n",
    "**Possible extra:** Write your preprocessing code as reusable functions, as you often have to preprocess multiple textual sources in a consistent way. You can then avoid code duplication in Question 8.\n",
    "\n",
    "**Possible extra:** Plot the relative frequency of the top N words (e.g., use the Pareto chart you did above, or another suitable plot) and discuss whether the distribution might follow the [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "**Possible extra:** Add lemmatization or stemming and part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (10 points)\n",
    "\n",
    "Do some self-learning: implement the same pipeline of question 6 using [spaCy pipelines](https://spacy.io/usage/processing-pipelines).\n",
    "\n",
    "Hint: Make sure to know what Spacy does by default when loading specific models, the default options are not always what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive text analysis (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (20 points)\n",
    "\n",
    "In the `data/numan` directory, there are lyrics of some songs from two albums by electronic music pioneer Gary Numan. There are 5 songs from his 1979 album and 5 songs from his 2017 album.  This data was acquired from [Genius](https://genius.com) (Genius Lyrics) using their API, something that you could do too using the lyricsgenius package for Python!\n",
    "\n",
    "- Load the data from these files into an appropriate data structure and perform appropriate preprocessing for this type of data (which might be different from preprocessing old books). Motivate your choices\n",
    "\n",
    "**Possible extra:** This might be a good opportunity to show how good you were at writing re-usable code in Question 6. Avoid duplication of code by calling back to functions you made in Question 6. Only include the steps that also make sense for song lyrics.\n",
    "\n",
    "- Write a function that can return some statistics about an albums' song lyrics, and run this function for both albums:\n",
    "\n",
    "    * Most frequent words\n",
    "    * Type to token ratio (unique words/words)\n",
    "    * Average word length\n",
    "    * Longest and shortests songs (by lyrics)\n",
    "    * What are the songs with the largest vocabulary and smallest vocabulary?\n",
    "    \n",
    "- Print these results to your notebook for both albums in a nice looking way. Then, also show these same statistics for just the song 'Cars' from the 1979 album, which was Gary Numan's most famous song.\n",
    "   \n",
    "   * Write down your interpretation of these results in this notebook.  \n",
    "   * In which of the two time periods was Gary Numan more verbose? Back it with some evidence.\n",
    "   * Electronic music is sometimes said to make more use of repetition than other forms of music. In which of the two time periods did Gary Numan make more use of lyrical repetition? You can either argue your case based on the numbers you were asked to calculate, or you can come up with your own definition of 'repetitiveness' and calculate it with Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
