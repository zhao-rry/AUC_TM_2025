{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQzYGMVl4-1n"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dloey_mk4-1q"
   },
   "source": [
    "## Guidelines\n",
    "\n",
    "> Remember that this is a code notebook - add an explanation of what you do using text boxes and markdown, and comment your code. Answers without explanations may get less points.\n",
    ">\n",
    "> If you re-use a substantial portion of code you find online, e.g on Stackoverflow, you need to add a link to it and make the borrowing explicit. The same applies of you take it and modify it, even substantially. There is nothing bad in doing that, providing you are acknowledging it and make it clear you know what you're doing.\n",
    ">\n",
    "> The **Generative AI policy** from the syllabus for the programming assignments applies. Generative AI can be used as a source of information in these assignments if properly referenced. You can use generative AI assistance for writing code, but you must reference the chat used as a source, just as if you would take from StackOverflow. In ChatGPT, you can make an URL to the information you obtained by clicking the \"Share link to Chat\" button and then \"Copy Link\". This allows you to cite the source of the information you use in your answer or code solution. Of course, as you know, GenAI tools are not always a reliable source and its answers are intransparantly drawn from other sources - it is recommended to cross-check its output with other sources or your own understanding of the topic.\n",
    "> \n",
    "> For the explanations of what you do that you provide with each question, as well as for (sub)questions that ask about things like motivation of choices or your opinion, the answer to this must be conceptualized and written by yourself and not copied from a generative AI source.\n",
    ">\n",
    "> Make sure your notebooks have been run when you submit, as I won't run them myself. Submit both the `.ipynb` file along with an `.html` export of the same. Submit all necessary auxilliary files as well. Please compress your submission into a `.zip` archive. Only `.zip` files can be submitted.\n",
    "> If you are using Google Colab, here is a tutorial for obtaining an HTML export: https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab .\n",
    ">\n",
    "> With Jupyter, you can simply export it as HTML through the File menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfy-TozC4-1s"
   },
   "source": [
    "## Grading policy\n",
    "> As follows:\n",
    ">\n",
    "> * 80 points for correctly completing the assignment.\n",
    ">\n",
    "> * 20 points for appropriately writing and organizing your code in terms of structure, readibility (also by humans), comments and minimal documentation. It is important to be concise but also to explain what you did and why, when not obvious. Feel free to re-use functions and variables from previous questions if that helps for structure and readability - you do not need to repeat previous steps for each question.\n",
    "> \n",
    "> Note that there are no extras for this assignment, as all 100 points are accrued via questions and question 6 has 10 'advanced' points to get.\n",
    "\n",
    "**The AUC code of conduct applies to this assignment: please only submit your own work and follow the instructions on referencing external sources above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRnI9EF74-1t"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmAFtiS14-1u"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will build and compare classifiers for measuring the **sentiment of tweets related to COVID-19** from the early days of the first outbreak.\n",
    "\n",
    "The dataset you will work with is [publicly available in Kaggle](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification) (and attached to the assignment for your convenience). Make sure to check its minimal Kaggle documentation before starting.\n",
    "\n",
    "This is a real dataset, and therefore messy. It is possible that you won't achieve great results on the classification task with your classifier. That is normal, don't worry about it! You also may find text encoding issues with this dataset. Try to find a simple solution to this problem, I don't think there is an easy way to fix it completely for these files.\n",
    "\n",
    "*Please note: this dataset should not but might contain content which could be considered as offensive.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv4avAae4-1v"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTwyDa3M4-1v"
   },
   "source": [
    "# Skeleton pipeline (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXgFtpSY4-1x"
   },
   "source": [
    "## Question 1 (8 points)\n",
    "\n",
    "Your dataset contains tweets, including handlers, hashtags, URLs, etc. Set-up a **minimal pre-processing pipeline** for them (focus on the `OriginalTweet` column), possibly including:\n",
    "\n",
    "* Tokenization\n",
    "* Filtering\n",
    "* Lemmatization/Stemming\n",
    "\n",
    "Please note that what to include is up to you, motivate your choices and remember that more is not necessarily better: if you are not sure why you are doing something, it might be better not to. Feel free to use NLTK, spaCy or anything else you like here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcGyC86g4-13"
   },
   "source": [
    "*Note: we only really use the `OriginalTweet` and `Sentiment` columns for this assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AbGfZgBN4-14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       OriginalTweet           Sentiment\n",
      "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
      "1  advice Talk to your neighbours family to excha...            Positive\n",
      "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
      "3  My food stock is not the only one which is emp...            Positive\n",
      "4  Me, ready to go at supermarket during the #COV...  Extremely Negative\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41157 entries, 0 to 41156\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   OriginalTweet  41157 non-null  object\n",
      " 1   Sentiment      41157 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 643.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path.cwd() / \"data/Corona_NLP_train.csv\"\n",
    "\n",
    "# the file is not UTF-8 encoded, so change encoding\n",
    "df_data = pd.read_csv(DATA_PATH, encoding=\"latin1\")\n",
    "df_data = df_data[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "print(df_data.head())\n",
    "print(df_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       OriginalTweet           Sentiment\n",
      "0  menyrbie philgahan chrisitv httpstcoifzfan and...             Neutral\n",
      "1  advice talk your neighbours family exchange ph...            Positive\n",
      "2  coronavirus australia woolworths give elderly ...            Positive\n",
      "3  food stock not the only one which empty   plea...            Positive\n",
      "4  ready supermarket during the covid outbreak  n...  Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\",\n",
    "                exclude=[\"tok2vec\", \"parser\", \"ner\"])\n",
    "\n",
    "texts = df_data[\"OriginalTweet\"]\n",
    "\n",
    "def doc_to_string(doc) -> str:\n",
    "    \"\"\"\n",
    "    Reformats strings returned by the SpaCy pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    s = \"\"\n",
    "    for token in doc:\n",
    "        if len(str(token)) > 2:\n",
    "            s += ''.join([\n",
    "                char for char in str(token).lower()\n",
    "                if char.isalpha()\n",
    "            ]) + ' '\n",
    "\n",
    "    return s.rstrip()\n",
    "\n",
    "docs = [doc_to_string(doc) for doc in nlp.pipe(texts)]\n",
    "\n",
    "df_data[\"OriginalTweet\"] = docs\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeR26J-N4-15"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyDE26-i4-15"
   },
   "source": [
    "## Question 2 (4 points)\n",
    "\n",
    "**Split your data into a train and a validation set**. You can use 85% for training and 15% for validation, or similar proportions. Remember to shuffle your data before splitting, specifying a seed to be able to replicate your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9vWDVkRB4-16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31486,) (5556,) (4115,)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_test, X_nottest,\n",
    " y_test, y_nottest) = train_test_split(\n",
    "     df_data[\"OriginalTweet\"], df_data[\"Sentiment\"], test_size=0.9, random_state = 888\n",
    " )\n",
    "\n",
    "(X_val, X_train,\n",
    " y_val, y_train) = train_test_split(\n",
    "     X_nottest, y_nottest, test_size=0.85, random_state = 777\n",
    " )\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpfNtdX04-16"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I328Auig4-17"
   },
   "source": [
    "## Question 3 (8 points)\n",
    "\n",
    "Write a function which, given as input a set of predictions and a set of ground truth labels and the name of the method, prints out a **classification report** including:\n",
    "* Name of the method\n",
    "* Accuracy\n",
    "* Precision, recall and F1 measure\n",
    "* An example of a correctly classified datapoint (e.g. a tweet)\n",
    "* An example of a wrongly classified datapoint\n",
    "\n",
    "*Note: You can do this question at the same time as question 4 so that you have something to report (the result of the baseline)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uGTHtBPu4-18"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def classification_report(pred, truth, model: str):\n",
    "    \"\"\"\n",
    "    Prints a classification report for a given method\n",
    "    \"\"\"\n",
    "    # LABELS = np.unique(truth)\n",
    "    labels = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]\n",
    "\n",
    "    print(f\"Results for {model}\")\n",
    "\n",
    "    total_correct = np.sum(pred == truth)\n",
    "    accuracy = total_correct / len(truth)\n",
    "    print(f\"Accuracy:   {round(accuracy*100, 1)}%\")\n",
    "    \n",
    "    cm = confusion_matrix(truth, pred, labels=labels)\n",
    "    # print(labels, cm)\n",
    "\n",
    "    # rows = underlying class\n",
    "    # cols = labelled class\n",
    "    row_sums = np.sum(cm, axis = 1)\n",
    "    col_sums = np.sum(cm, axis = 0)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"\\nScores for class \\\"{label}\\\"\")\n",
    "        \n",
    "        t_pos = cm[i, i]\n",
    "        f_neg = row_sums[i] - t_pos\n",
    "        f_pos = col_sums[i] - t_pos\n",
    "        t_neg = total_correct - t_pos\n",
    "        \n",
    "        precision = t_pos / (t_pos + f_pos)\n",
    "        recall = t_pos / (t_pos + f_neg)\n",
    "        f_measure = 2 * ((precision * recall) / (precision + recall))\n",
    "        print(f\"Precision:  {round(precision * 100, 1)}%\")\n",
    "        print(f\"Recall:     {round(recall * 100, 1)}%\")\n",
    "        print(f\"F1 Measure: {round(f_measure * 100, 1)}%\")\n",
    "\n",
    "    # more to do but i'm tired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZG3nFTa4-18"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwLrjSBV4-18"
   },
   "source": [
    "# Classifying (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will be performing classification on real data, processes may take a while to run. This is normal, but it should not take hours. Here's some advice if you find that some of your code takes a long time to run:\n",
    "- If you are doing a hyperparameter search, try to make it quite small. Every hyperparameter combination that you try means training a new model, and runtimes can explode. You do not need to do a huge search for this assignment, it is enough if I can see that you are able to do it with a small example.\n",
    "- If you are doing a grid search, try to know how many combinations of hyperparameters your code will check and try to have print statements to know where you are at. Computation time grows exponentially for each additional hyperparameter option so this can get out of hand quickly. Also, if training a single model as a step of your grid search takes longer than just training the model separately, there might be an issue with your grid search code.\n",
    "- In a real project, you would want to make your code such that you can pause and resume training or optimization without having to re-do everything, e.g. by writing the results to a file. But for the purpose of this assignment it is not necessary to make it that complicated.\n",
    "- Use separate code blocks especially for the part of code that trains a model. That way, you only need to run the training step once, while you can mess around with the output/evaluation etc. without having to wait for a new model to be trained each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_gI9lIS4-19"
   },
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "An important first step when dealing with a real-world task is establishing a **solid baseline**. The baseline allows to a) develop the first full pipeline for your task, and b) to have something to compare against when you develop more advanced models.\n",
    "\n",
    "Pick a method to use as a baseline. *A good option might be a TF-IDF Logistic Regression*. Feel free to use scikit-learn or another library of choice. See [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for more options.\n",
    "\n",
    "Use your classification report function and the validation set to report on the performance of your baseline. *Pay attention: the validation data only needs to be transformed, and must not be used to fit any transformation. For example, if you have used a TF-IDF vectorizer by fitting it to your train data and then transformed it, use the same fitted vectorizer to transform your validation data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DOyDJ8494-1-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF Logistic Regression\n",
      "Accuracy:   56.3%\n",
      "\n",
      "Scores for class \"Extremely Negative\"\n",
      "Precision:  64.3%\n",
      "Recall:     49.4%\n",
      "F1 Measure: 55.8%\n",
      "\n",
      "Scores for class \"Negative\"\n",
      "Precision:  49.2%\n",
      "Recall:     51.5%\n",
      "F1 Measure: 50.3%\n",
      "\n",
      "Scores for class \"Neutral\"\n",
      "Precision:  64.0%\n",
      "Recall:     64.8%\n",
      "F1 Measure: 64.4%\n",
      "\n",
      "Scores for class \"Positive\"\n",
      "Precision:  50.9%\n",
      "Recall:     60.2%\n",
      "F1 Measure: 55.1%\n",
      "\n",
      "Scores for class \"Extremely Positive\"\n",
      "Precision:  67.0%\n",
      "Recall:     52.5%\n",
      "F1 Measure: 58.8%\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TFIDF(\n",
    "    max_df=0.5,\n",
    "    min_df=2\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val) # do not fit!\n",
    "\n",
    "tfidf_logreg = LogisticRegression(solver=\"sag\")\n",
    "tfidf_logreg.fit(X_train_tfidf, y_train)\n",
    "# no transformation needed for logistic regression\n",
    "y_pred_tfidf_logreg = tfidf_logreg.predict(X_val_tfidf)\n",
    "\n",
    "classification_report(y_pred_tfidf_logreg, y_val, \"TF-IDF Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> EXPLANATION </h3>\n",
    "\n",
    "why are we using logistic regression when there are more than two classes?!?! am i supposed to do something else...\n",
    "\n",
    "use sag because uhhh lbfgs reaches iteration limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnnmO3Kf4-1-"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOE_ZHLv4-1-"
   },
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Try now to **beat your baseline**. Feel free to use scikit-learn or another library of choice. See [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for more options.\n",
    "\n",
    "How to beat the baseline? There are many ways:\n",
    "1. You could have a better text representation (e.g., using PPMI instead of TF-IDF, note that this is challenging because there is no ready-made scikit-learn vectorizer for this).\n",
    "2. You can pick a more powerful model (e.g., random forests or SVMs).\n",
    "3. You have to find good hyperparameters for your model, and not just use the default ones.\n",
    "\n",
    "Regarding point 3 above, make sure to perform some hyperparameter searching using [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or [randomized search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "Use your classification report function and the validation set to report on the performance of your baseline. *Pay attention: the validation data only needs to be transformed, and must not be used to fit any transformation. For example, if you have used a TF-IDF vectorizer by fitting it to your train data and then transformed it, use the same fitted vectorizer to transform your validation data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pIcbBVMO4-1_"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9zmQoa74-1_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV-1eX7y4-1_"
   },
   "source": [
    "## Question 6 (15 points)\n",
    "\n",
    "Design, develop and train a **neural network-based classifier** for this task, using scikit-learn, PyTorch or the Transformers library. The scikit-learn approach is demonstrated in Notebook 7_1, the Pytorch approach is demonstrated in Notebook 7_2. The Transformers approach is the most state-of-the-art approach, which involves taking a pre-trained LLM and tuning a sequence classification head for your text classification task. You can find a basic example in the Huggingface documentation: https://huggingface.co/docs/transformers/en/tasks/sequence_classification\n",
    "\n",
    "The scikit-learn option is probably simpler than you think. Pytorch and Transformers classifiers are more advanced and challenging, but due to the current popularity of Transformer models it is relatively easy to find solutions to your problems with Transformer models. If you are up for a challenge, choose Pytorch if you are more interested in foundations of neural networks and machine learning more broadly, or choose Transformers if you are interested in LLMs and textual data.\n",
    "\n",
    "The classifier can have the structure that you prefer and use an embedding model of your choice, just make sure to motivate your choices.\n",
    "\n",
    "*Note: an NN-based classifier with scikit-learn yields 5 points max; one with PyTorch or a pre-tuned Transformers-based model yields 10 points max; one with PyTorch and pre-trained embeddings or a Transformers-based model tuned by yourself yields 15 points max. If you try PyTorch or Transformers but get stuck, you can still get partial points if you have a good explanation of what you tried.*\n",
    "\n",
    "Use your classification report function and the validation set to report on the performance of your baseline. *Pay attention: the validation data only needs to be transformed, and must not be used to fit any transformation. For example, if you have used a TF-IDF vectorizer by fitting it to your train data and then transformed it, use the same fitted vectorizer to transform your validation data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tDn5kJj54-2A"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8K-n8Ak4-2A"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjiTUFSZ4-2A"
   },
   "source": [
    "# Evaluating your classifiers (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9JXwtnF4-2B"
   },
   "source": [
    "## Question 7 (8 points)\n",
    "\n",
    "Evaluate the performance of your models on the **test set**. Make sure to transform your test data as you did for your train data, and as needed for each classifier. *Pay attention: the test data only needs to be transformed, and must not be used to fit any transformation. For example, if you have used a TF-IDF vectorizer by fitting it to your train data and then transformed your train and validation with it, use the same fitted vectorizer to transform your test data.*\n",
    "\n",
    "* Report the accuracy of each classifier, as well as its precision, recall and F1 score. \n",
    "* Plot a [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) for your best classifier.\n",
    "* Briefly discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FDr66A_j4-2B"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/Corona_NLP_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WFviO9dr4-2B"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRm9J4-o4-2C"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxvAsxg4-2C",
    "tags": []
   },
   "source": [
    "## Question 8 (7 points)\n",
    "\n",
    "When you perform a classification or labeling task, you may want to perform an error analysis to look for avenues for improvement. You can do this both quantitatively and qualitatively.\n",
    "\n",
    "For your best classifier:\n",
    "* Collect misclassified samples, e.g. by modifying your evaluation code from Question 7.\n",
    "\n",
    "Perform a brief quantitative error analysis of your best classifier:\n",
    "* Choose some properties that you think are relevant to classification quality, such as the length of the tweet or use of emoji. Come up with three interesting properties.\n",
    "* Compute and compare these three properties for the misclassified samples to the average distribution over all samples.\n",
    "* Describe your conclusions.\n",
    "\n",
    "Perform a brief qualitative error analysis of your best classifier:\n",
    "* Look at the misclassified samples, and make observations about their properties. Identify some properties that you think are relevant to classification quality but that you can't easily quantify, such as usage of sarcasm or irony, negation issues (not bad != bad), spelling or grammar issues, interpretation of emojis, context dependence of the tweet, or other observations.\n",
    "* Describe your conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ONz1PmOM4-2E"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2z2g9eF4-2E"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_a3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
