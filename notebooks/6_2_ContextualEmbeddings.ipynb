{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXqBicFe_3Dc"
   },
   "source": [
    "# Contextual Word Embeddings with ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parts of this notebook are based on the DutchSimLex code of Lizzy Brans: https://github.com/lizzybrans/Simlex999-Dutch. Thanks to her!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMt7yyaD_3Df"
   },
   "source": [
    "We will perform the same type of word embedding evaluation from last week, but now with ModernBERT, a recent bidirectional encoder LLM that is an updated version of BERT. We need a recent version of the Transformers library (version 4.48.0 or newer) to be able to use this. You may have to install a bunch of recent Transformers library stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qbs1UmP9h3OO"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, ModernBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "model = ModernBertModel.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOS55Ni4_3Dg",
    "tags": []
   },
   "source": [
    "### Other models\n",
    "\n",
    "See here for pre-trained models available via Huggingface: https://huggingface.co/models\n",
    "\n",
    "Here is a list of fill-mask models similar to BERT: https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj48kEcu_3Dm"
   },
   "source": [
    "### Training a model\n",
    "\n",
    "We aren't going to attempt training a BERT-based model from scratch this time, this requires a lot of compute and data. Typically you use a pre-trained model, and maybe tune it. You can find a variety of models for most major languages on Huggingface and use them with the Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding single words in a contextual embedding model\n",
    "\n",
    "Let's define a function that can get am embedding from a specific layer for a specific word, while also subtokenizing it and taking the average of the subtokens as the embedding for the word. We also define a function to calculate similarity between a word pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, layer_nums):\n",
    "    # Tokenize the word into subtokens and add special tokens [CLS] and [SEP]\n",
    "    subtokens = [tokenizer.cls_token] + tokenizer.tokenize(word) + [tokenizer.sep_token]\n",
    "    # Convert subtokens to input IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(subtokens)\n",
    "    # Wrap it in a tensor and add an extra batch dimension\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    # Make sure the model does not compute gradients\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    # Check if layer_nums is a list or a single integer\n",
    "    if isinstance(layer_nums, int):\n",
    "        layer_nums = [layer_nums]\n",
    "    # Use the hidden state from the specified layers as word embedding\n",
    "    embeddings = [outputs.hidden_states[i] for i in layer_nums]\n",
    "    # Average the embeddings from the specified layers\n",
    "    averaged_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    # Ignore the first and the last token ([CLS] and [SEP])\n",
    "    averaged_embedding = averaged_embedding[0, 1:-1]\n",
    "    # Get the mean of the subtoken vectors to get the word vector\n",
    "    word_embedding = torch.mean(averaged_embedding, dim=0)\n",
    "    # Convert tensor to a numpy array\n",
    "    word_embedding = word_embedding.numpy()\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(word1, word2, layer_nums):\n",
    "    word1_embedding = get_word_embedding(word1, layer_nums)\n",
    "    word2_embedding = get_word_embedding(word2, layer_nums)\n",
    "    similarity = 1 - cosine(word1_embedding, word2_embedding)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform the same kinds of similarity queries as last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3RZeTMb_3Di",
    "outputId": "31cda8fe-b416-4366-d982-05008a5a73bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.67\n",
      "'car'\t'bicycle'\t0.69\n",
      "'car'\t'airplane'\t0.65\n",
      "'car'\t'cereal'\t0.12\n",
      "'car'\t'communism'\t0.04\n"
     ]
    }
   ],
   "source": [
    "# similarity queries (default to cosine similarity: 0 least similar, to 1 most similar)\n",
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, calculate_similarity(w1, w2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to get similarities from layer 0, as this typically works best for words without context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Let's once again try to evaluate this model using the wordSim-353 benchmark, as we did in Notebook 4. This time, we don't need to select words for which we have enough data - the model should be able to embed all words in the benchmark.\n",
    "\n",
    "This may take a while, as it will embed all words into this large and modern LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSim353 = dict()\n",
    "\n",
    "with open(\"data/wordSim353.csv\",\"r\") as infile:\n",
    "    next(infile) #skip header\n",
    "    for line in infile:\n",
    "        raw_w1, raw_w2, rating = line.strip().split(\",\")\n",
    "        w1 = raw_w1+\"-NOUN\"\n",
    "        w2 = raw_w2+\"-NOUN\"\n",
    "        wordSim353[(w1, w2)] = float(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = []\n",
    "measures = []\n",
    "\n",
    "layer_num = 0\n",
    "\n",
    "print(\"ModernBERT-based space vs. wordSim353 -> spearman's rho:\\t\", )\n",
    "\n",
    "wordSim_ratings = []\n",
    "vsm_sims_l0 = []\n",
    "\n",
    "for idx, ((w1, w2), r) in enumerate(wordSim353.items()):\n",
    "    wordSim_ratings.append(r)\n",
    "    vsm_sims_l0.append(calculate_similarity(w1, w2, layer_num))\n",
    "    \n",
    "    #Let's add a progress counter as this takes a while.\n",
    "    if idx % 50 == 0 and idx > 0:\n",
    "        print(f\"Embedded {idx} word pairs...\")\n",
    "\n",
    "rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims_l0)\n",
    "\n",
    "print(rho)\n",
    "rhos.append(rho)\n",
    "measures.append(\"ModernBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0, len(rhos)), rhos)\n",
    "plt.xticks(range(0, len(rhos)), measures, size='small', rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate each layer of the model, and visualize the results. Note that this will take 23 times longer than the previous part. If you don't want to wait for that, you can skip to the \"Error Analysis\" part, which only uses the previous result from layer 0.\n",
    "\n",
    "This is why we don't do many exercises with modern LLMs in this course..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Spearman correlation for each layer\n",
    "rho_layers = []\n",
    "\n",
    "for layer_num in range(23):  # For ModernBERT base models, there are 23 layers including the output layer\n",
    "    wordSim_ratings = []  # Initialize similarity_scores in each iteration\n",
    "    vsm_sims = []\n",
    "\n",
    "    for (w1, w2), r in wordSim353.items():\n",
    "        similarity = calculate_similarity(w1, w2, layer_num)\n",
    "        vsm_sims.append(similarity)\n",
    "        wordSim_ratings.append(r)\n",
    "\n",
    "    rho, pval = scipy.stats.spearmanr(wordSim_ratings, vsm_sims)\n",
    "    rho_layers.append(rho)\n",
    "\n",
    "    print(f'Layer {layer_num} - Spearman correlation: {rho}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting in blue template\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.barplot(x=list(range(23)), y=rho_layers, palette=\"Blues_d\", ax=ax, edgecolor='black')\n",
    "ax.set_title('Spearman Correlation Across Transformer Layers', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Transformer Layers', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Spearman Correlation', fontsize=12, fontweight='bold')\n",
    "ax.grid(True)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does worse than we might expect. Let's perform an error analysis of the layer 0 results to see where the largest differences between the model and human ratings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use a dataframe\n",
    "wordsim = pd.DataFrame(wordSim353.items(), columns=['pair', 'rating-wordsim'])\n",
    "wordsim['predicted-ModernBERT'] = vsm_sims_l0\n",
    "wordsim['predicted-ModernBERT'] = wordsim['predicted-ModernBERT']*10 #rescale the predictions to be 0-10\n",
    "wordsim['abs_diff-ModernBERT'] = abs(wordsim['rating-wordsim'] - wordsim['predicted-ModernBERT'])\n",
    "largest_diff = wordsim.nlargest(10, 'abs_diff-ModernBERT')\n",
    "print(largest_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL9eE7Hu_3Dp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhzaTWQ9_3Dp"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Sometimes, it is possible to get better results by summing the embeddings of multiple layers. Can you improve on this result by finding some layer combination that correlates better with the human benchmark?\n",
    "2. Try to evaluate some other fill-mask models from https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending, such as BERT (the classic), RoBERTa or XLNet. Are any of them better at this task than the more recent ModernBERT? They'll certainly be faster.\n",
    "3. Try to evaluate using the SimLex-999 benchmark, instead of wordSim-353."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DaQOZhk8_3Dp"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5_WordEmbeddings_gensim4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
